 from _future_ import print_function 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import classification_report 
from sklearn import metrics 
from sklearn import tree 
df=pd.read_csv('/content/new.csv') 
df.head() 
print('A sample set of rows for dataframe is:\n') 
display(df.sample(6)) 
df.size 
df.shape 
df.columns 
df['label'].unique() 
df.dtypes 
df['label'].value_counts() 
df.isnull().sum() 
df.info() 
df.describe() 
crop_summary = pd.pivot_table(df,index=['label'],aggfunc='mean') 
crop_summary.head() 
plt.style.use('ggplot') 
sns.set_palette("husl") 
for i in df.columns[:-1]: 
fig,ax=plt.subplots(1,3,figsize=(18,4)) 
sns.histplot(data=df,x=i,kde=True,bins=40,ax=ax[0]) 
sns.violinplot(data=df,x=i,ax=ax[1]) 
sns.boxplot(data=df,x=i,ax=ax[2]) 
plt.suptitle(f'Visualizing {i}',size=20) 
grouped = df.groupby(by='label').mean().reset_index() 
grouped 
print(f'') 
for i in grouped.columns[1:]: 
print(f'Top 5 Most {i} requiring crops:') 
print(f'') 
for j ,k in grouped.sort_values(by=i,ascending=False)[:5][['label',i]].values: 
print(f'{j} --> {k}') 
print(f'') 
fig = go.Figure() 
fig.add_trace(go.Bar( 
x=crop_summary.index, 
y=crop_summary['N'], 
name='Nitrogen', 
marker_color='indianred' 
)) 
fig.add_trace(go.Bar( 
x=crop_summary.index, 
y=crop_summary['P'], 
name='Phosphorous', 
marker_color='lightsalmon' 
)) 
fig.add_trace(go.Bar( 
x=crop_summary.index, 
y=crop_summary['K'], 
name='Potash', 
marker_color='crimson' 
)) 
fig.update_layout(title="N, P, K values comparision between crops", 
plot_bgcolor='white', 
barmode='group', 
xaxis_tickangle=-45) 
fig.show() 
sns.heatmap(df.corr(numeric_only=True),annot=True) 
features = df[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']] 
target = df['label'] 
labels = df['label'] 
# Initializing empty lists to append all model's name and corresponding name 
acc = [] 
model = [] 
# Splitting into train and test data 
from sklearn.model_selection import train_test_split 
Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =2) 
from sklearn.tree import DecisionTreeClassifier 
DecisionTree = DecisionTreeClassifier(criterion="entropy",random_state=2,max_depth=5) 
DecisionTree.fit(Xtrain,Ytrain) 
predicted_values = DecisionTree.predict(Xtest) 
x = metrics.accuracy_score(Ytest, predicted_values) 
acc.append(x) 
model.append('Decision Tree') 
print("DecisionTrees's Accuracy is: ", x*100) 
print(classification_report(Ytest,predicted_values)) 
from sklearn.model_selection import cross_val_score 
# Cross validation score (Decision Tree) 
score = cross_val_score(DecisionTree, features, target,cv=5) 
score 
import pickle 
# Dump the trained Naive Bayes classifier with Pickle 
DT_pkl_filename = 'DecisionTree.pkl' 
# Open the file to save as pkl file 
DT_Model_pkl = open(DT_pkl_filename, 'wb') 
pickle.dump(DecisionTree, DT_Model_pkl) 
# Close the pickle instances 
DT_Model_pkl.close() 
from sklearn.naive_bayes import GaussianNB 
NaiveBayes = GaussianNB() 
NaiveBayes.fit(Xtrain,Ytrain) 
predicted_values = NaiveBayes.predict(Xtest) 
x = metrics.accuracy_score(Ytest, predicted_values) 
acc.append(x) 
model.append('Naive Bayes') 
print("Naive Bayes's Accuracy is: ", x) 
print(classification_report(Ytest,predicted_values)) 
# Cross validation score (NaiveBayes) 
score = cross_val_score(NaiveBayes,features,target,cv=5) 
score 
import pickle 
# Dump the trained Naive Bayes classifier with Pickle 
NB_pkl_filename = 'NBClassifier.pkl' 
# Open the file to save as pkl file 
NB_Model_pkl = open(NB_pkl_filename, 'wb') 
pickle.dump(NaiveBayes, NB_Model_pkl) 
# Close the pickle instances 
NB_Model_pkl.close() 
from sklearn.svm import SVC 
SVM = SVC(gamma='auto') 
SVM.fit(Xtrain,Ytrain) 
predicted_values = SVM.predict(Xtest) 
x = metrics.accuracy_score(Ytest, predicted_values) 
acc.append(x) 
model.append('SVM') 
print("SVM's Accuracy is: ", x) 
print(classification_report(Ytest,predicted_values)) 
# Cross validation score (SVM) 
score = cross_val_score(SVM,features,target,cv=5) 
score 
from sklearn.linear_model import LogisticRegression 
LogReg = LogisticRegression(random_state=2) 
LogReg.fit(Xtrain,Ytrain) 
predicted_values = LogReg.predict(Xtest) 
x = metrics.accuracy_score(Ytest, predicted_values) 
acc.append(x) 
model.append('Logistic Regression') 
print("Logistic Regression's Accuracy is: ", x) 
print(classification_report(Ytest,predicted_values)) 
# Cross validation score (Logistic Regression) 
score = cross_val_score(LogReg,features,target,cv=5) 
score 
import pickle 
# Dump the trained Naive Bayes classifier with Pickle 
LR_pkl_filename = 'LogisticRegression.pkl' 
# Open the file to save as pkl file 
LR_Model_pkl = open(DT_pkl_filename, 'wb') 
pickle.dump(LogReg, LR_Model_pkl) 
# Close the pickle instances 
LR_Model_pkl.close() 
from sklearn.ensemble import RandomForestClassifier 
RF = RandomForestClassifier(n_estimators=20, random_state=0) 
RF.fit(Xtrain,Ytrain) 
predicted_values = RF.predict(Xtest) 
x = metrics.accuracy_score(Ytest, predicted_values) 
acc.append(x) 
model.append('RF') 
print("RF's Accuracy is: ", x) 
print(classification_report(Ytest,predicted_values)) 
# Cross validation score (Random Forest) 
score = cross_val_score(RF,features,target,cv=5) 
score 
import pickle 
# Dump the trained Naive Bayes classifier with Pickle 
RF_pkl_filename = 'RandomForest.pkl' 
# Open the file to save as pkl file 
RF_Model_pkl = open(RF_pkl_filename, 'wb') 
pickle.dump(RF, RF_Model_pkl) 
# Close the pickle instances 
RF_Model_pkl.close() 
from sklearn.preprocessing import LabelEncoder 
import xgboost as xgb 
XB = xgb.XGBClassifier() 
# Create a label encoder 
le = LabelEncoder() 
# Fit the encoder on the unique values of y 
le.fit(np.unique(Ytrain)) 
# Transform the training and test labels 
Ytrain = le.transform(Ytrain) 
Ytest = le.transform(Ytest) 
# Now you can fit the XGBClassifier 
XB.fit(Xtrain, Ytrain) 
predicted_values = XB.predict(Xtest) 
x = metrics.accuracy_score(Ytest, predicted_values) 
acc.append(x) 
model.append('XGBoost') 
print("XGBoost's Accuracy is: ", x) 
print(classification_report(Ytest,predicted_values)) 
from sklearn.preprocessing import LabelEncoder 
# Create a label encoder 
le = LabelEncoder() 
# Fit the encoder on the unique values of target 
le.fit(np.unique(target)) 
# Transform the training and test labels 
target = le.transform(target) 
# Cross validation score (XGBoost) 
score = cross_val_score(XB,features,target,cv=5) 
score 
import pickle 
# Dump the trained Naive Bayes classifier with Pickle 
XB_pkl_filename = 'XGBoost.pkl' 
# Open the file to save as pkl file 
XB_Model_pkl = open(XB_pkl_filename, 'wb') 
pickle.dump(XB, XB_Model_pkl) 
# Close the pickle instances 
XB_Model_pkl.close() 
plt.figure(figsize=[10,5],dpi = 100) 
plt.title('Accuracy Comparison') 
plt.xlabel('Accuracy') 
plt.ylabel('Algorithm') 
sns.barplot(x = acc,y = model,palette='dark') 
accuracy_models = dict(zip(model, acc)) 
for k, v in accuracy_models.items(): 
print (k, '-->', v) 
data = np.array([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]]) 
prediction = RF.predict(data) 
print(prediction) 
data = np.array([[83, 45, 60, 28, 70.3, 7.0, 150.9]]) 
prediction = RF.predict(data) 
print(prediction)